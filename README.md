# Natural Language Processing with Transformers â€“ Hands-On

This repository contains hands-on implementations and experiments inspired by the book  
**"Natural Language Processing with Transformers"**.

Each chapter focuses on practical applications of transformer-based models, implemented through Jupyter notebooks to reinforce core NLP concepts and real-world workflows.

## ðŸ”§ Tech Stack
- ðŸ¤— Hugging Face Transformers
- PyTorch
- TensorFlow
- Jupyter Notebook

## ðŸ“‚ Contents
- Tokenization and embeddings
- Fine-tuning transformer models
- Text classification, NER, and sequence labeling
- Transfer learning with pretrained models
- Model evaluation and inference

## ðŸŽ¯ Goal
To build a strong practical understanding of modern NLP using transformers by combining theory with hands-on experimentation.

## ðŸš€ Notes
- Code is written for learning and experimentation.
- Notebooks are updated chapter-wise as progress continues.

---

Feel free to explore, fork, or use this repository as a learning reference.
